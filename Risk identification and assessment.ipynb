{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Um45nKhD-8Q9"},"outputs":[],"source":["pip install PyMuPDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOfYkfIm_HPa"},"outputs":[],"source":["pip install BERTopic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNYsAC2tFlEe"},"outputs":[],"source":["import os\n","import fitz\n","import nltk\n","import re\n","import string\n","import gensim\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import itertools\n","import pprint\n","\n","\n","# For tokenization\n","from nltk.tokenize import word_tokenize\n","nltk.download(\"punkt\")\n","nltk.download('wordnet')\n","\n","# For removing stopwords\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# For lemmatization\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","nltk.download('averaged_perceptron_tagger')\n","\n","# For TF-IDC\n","from gensim import corpora\n","from gensim.models import TfidfModel\n","\n","# For N-grams\n","from collections import defaultdict\n","from nltk.util import ngrams as nltk_ngrams\n","from nltk.tokenize import word_tokenize\n","\n","# For BERT topic\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","\n","# For coherence score\n","from gensim.models.coherencemodel import CoherenceModel"]},{"cell_type":"markdown","metadata":{"id":"tzm7k6FXFlEg"},"source":["### Get text from PDFs"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gs7u08foFlEh","executionInfo":{"status":"ok","timestamp":1716072759429,"user_tz":-180,"elapsed":567,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"}}},"outputs":[],"source":["def extract_text_from_pdf(pdf_path):\n","    text = \"\"\n","    with fitz.open(pdf_path) as pdf:\n","        for page_num in range(len(pdf)):\n","            page = pdf.load_page(page_num)\n","            text += page.get_text()\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"tiwzhA8vFlEh"},"source":["### Preprocess and vectorize text"]},{"cell_type":"markdown","metadata":{"id":"50jATjIvFlEh"},"source":["*Preprocess pipeline*"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sB2HEO01CD9g","executionInfo":{"status":"ok","timestamp":1716072761866,"user_tz":-180,"elapsed":2,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"}}},"outputs":[],"source":["# Loading WordNet POS tags for lemmatization\n","def wordnet_pos_tags(treebank_tag):\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716073785890,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"uc37TnTxFlEi"},"outputs":[],"source":["def txt_preprocess_pipeline(text):\n","\n","    # Removing multiple white spaces and line breaks\n","    clean_txt = re.sub(r'\\n', ' ', text)\n","    clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n","    clean_txt = clean_txt.strip()\n","\n","    # Tokenizing text\n","    tokens = nltk.word_tokenize(clean_txt)\n","\n","    # Loading NLTK stopword list and adding original stopwords\n","    stop_words = set(stopwords.words('english')+['impact','event','area','project','region','change','country','na','na na','city','oecd','urban','red','paper','forum','figgs','enfield','literature','estimate','model','effect','impact','extreme','study','tantly','treme','tsuji','tural','yt','en','ings','factsheet','edney','budin','apps','app','enbel','alth','aunan','basu','user','search','photo','tures','mistry','letter','cdds','ly','national','plan','hhaps','use','initiate','various','public','health','heat','temperature','change','risk', 'increase','health','india','plan', 'brutally','management','part','highlight','world','united','nation','joint', 'cooperation','brief','issue','professor','tsdps','ons','qlw','lfb','qh','qc','humber','osdma','hse','hm','maha','ucs','lok','international','doya','gsdma','cdri','eswd','dluhc','esd','dfe','niosh','naic','desnz','odisha','defra','easac','bouhi','awhp','ndma','rachel','bureau','scientist','adequate','annual','address','amplify','billion','calculation,','activate','adaptation','additionally','acknowledge','afternoon','accordance','act','accord','data','high','ukhsa','osha','mrt','donthu','ensemble','kristina','webinar','qr','percent','interviewee','licker','eea','ccc','conv','january','iiphb','ferguson','ulbrich','trenberth','schär','rcms','nao','germi','veisz','spell','sres','srex','phenomenon','probability','ρ','qsw','moj','nhs','imd','october','academicians','academic','absorb','aapda','nrdc','report','online','across','action','expand','page','u','https','j','g','figure','thank','you','mbove','print','info','escalate','isbn','volume','dr','vol','phd','upon','al', 'e', 'fig', 'cl', 'hap', 'et', 'http', 'ew', 'climate','jun','simon', 'yang','go','copyright','ltd','license','creative','imply','dot', 'dashed','line', 'may', 'either','bi','de', 'la', 'paix', 'tel', 'box', 'fax','front', 'oli', 'scarff', 'getty', 'image', 'back', 'atlas','ness','net', 'san', 'felipe','also','informa', 'tion', 'per', 'cent','due','distri', 'bution','com','bin','thus','km', 'grid','would','christopher','u','aa','aap','aas','ab','abares','abatzoglou','abbs','abc','aberystwyth','doi'])\n","\n","    # Adding custom stopwords directly to the set\n","    custom_stopwords = set(['impact','event','area','project','region','change','country','na','na na','city','oecd','urban','red','paper','forum','figgs','enfield','literature','estimate','model','effect','impact','extreme','study','tantly','treme','tsuji','tural','yt','en','ings','factsheet','edney','budin','apps','app','enbel','alth','aunan','basu','user','search','photo','tures','mistry','letter','cdds','ly','national','plan','hhaps','use','initiate','various','public','health', 'heat', 'temperature', 'change', 'risk', 'increase','health','india','plan', 'brutally','management','part','highlight','world','united','nation','joint', 'cooperation','brief','issue','professor','tsdps','ons','qlw','lfb','qh','qc','humber','osdma','hse','hm','maha','ucs','lok','international','doya','gsdma','cdri','eswd','dluhc','esd','dfe','niosh','naic','desnz','odisha','defra','easac','bouhi','awhp','ndma','rachel','bureau','scientist','adequate','annual','address','amplify','billion','calculation,','activate','adaptation','additionally','acknowledge','afternoon','accordance','act','accord','data','high','ukhsa','osha','mrt','donthu','ensemble','kristina','webinar','qr','percent','interviewee','licker','eea','ccc','conv','january','iiphb','ferguson','ulbrich','trenberth','schär','rcms','nao','germi','veisz','spell','sres','srex','phenomenon','probability','ρ','qsw','moj','nhs','imd','october','academicians','academic','absorb','aapda','nrdc','report','online','across','action','expand','page','u','https','j','g','figure','thank','you','mbove','print','info','escalate','isbn','volume','dr','vol','phd','upon','al', 'e', 'fig', 'cl', 'hap', 'et', 'http', 'ew', 'climate','jun','simon', 'yang','go','copyright','ltd','license','creative','imply','dot', 'dashed','line', 'may', 'either','bi','de', 'la', 'paix', 'tel', 'box', 'fax','front', 'oli', 'scarff', 'getty', 'image', 'back', 'atlas','ness','net', 'san', 'felipe','also','informa', 'tion', 'per', 'cent','due','distri', 'bution','com','bin','thus','km', 'grid','would','christopher','u','aa','aap','aas','ab','abares','abatzoglou','abbs','abc','aberystwyth','doi'])\n","    stop_words.update(custom_stopwords)\n","\n","    # Standardize the text and remove non-alphabetic or stop words\n","    words = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n","\n","    # Defining lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","\n","    # Conducting POS tagging\n","    pos_tags = nltk.pos_tag(words)\n","\n","    # Lemmatizing word-tokens via assigned POS tags\n","    lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]\n","\n","    return lemma_tokens"]},{"cell_type":"markdown","metadata":{"id":"8UFJhhzuFlEk"},"source":["*Iteration function*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45Seszr4OTO9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTrawQzXFlEk"},"outputs":[],"source":["def iterate_pdf_files(pdf_folder):\n","    # Creating a dictionary to display all documents and tokens together\n","    docs = {}\n","\n","    # Counting to tracking the number of processed texts\n","    text_count = 0\n","\n","    # Iterating over each file in the folder\n","    for filename in os.listdir(pdf_folder):\n","\n","        # Checking whether the files are PDFs\n","        if filename.endswith('.pdf'):\n","\n","            # Creating a path to the PDF files\n","            pdf_path = os.path.join(pdf_folder, filename)\n","\n","            # Extracting the content of files\n","            text = extract_text_from_pdf(pdf_path)\n","\n","            # Applying the preprocessing pipeline to the extracted tex\n","            preprocessed_text = txt_preprocess_pipeline(text)\n","\n","            # Generating a unique name for the document based on the length of the docs dictionary for ease in readibility\n","            doc_name = f'doc{len(docs) + 1}'\n","\n","            # Assigning the preprocessed text content of the current document in the docs dictionary with the document name as the key\n","            docs[doc_name] = [preprocessed_text]\n","\n","            # Increasing the text count when finished assigning\n","            text_count += 1\n","\n","    return docs\n","\n","pdf_folder = '/content/drive/MyDrive/risk'\n","\n","docs = iterate_pdf_files(pdf_folder)\n","\n","# Printing each document's tokens as a list\n","for doc_name, tokens in docs.items():\n","    print(f\"{doc_name}: {tokens}\")"]},{"cell_type":"markdown","metadata":{"id":"3OYTkISMFlEl"},"source":["*Token dictionary*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8giyHjwFlEl"},"outputs":[],"source":["# Iterating over the values in the docs dictionary created in the previous step. Each value in the sublist represents the tokens in a certain document.\n","all_preprocessed_text = [token for sublist in docs.values() for token in sublist]\n","\n","# Creating a gensim dictionary to assign an integer ID to each token\n","dictionary = corpora.Dictionary(all_preprocessed_text)\n","print(dictionary)"]},{"cell_type":"markdown","metadata":{"id":"89RhweuBFlEl"},"source":["*BoW representation*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06P76cTgFlEm"},"outputs":[],"source":["# Creating a Bag-of-Words (BoW) representation of the preprocessed text data using the dictionary created in the previous step\n","bow_corpus = [dictionary.doc2bow(text) for text in all_preprocessed_text]\n","pprint.pprint(bow_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16hEUyYZFlEm"},"outputs":[],"source":["print('Number of unique tokens: %d' % len(dictionary))\n","print('Number of documents: %d' % len(bow_corpus))"]},{"cell_type":"markdown","metadata":{"id":"kBkh_rHcFlEm"},"source":["### TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXRqzOxmFlEm"},"outputs":[],"source":["# Creating a TF-IDF model from the Bag-of-Words corpus\n","tfidf_model = TfidfModel(bow_corpus)\n","\n","# Applying the TF-IDF model to the Bag-of-Words corpus to get the TF-IDF representation\n","tfidf_corpus = tfidf_model[bow_corpus]\n","\n","# Printing the TF-IDF representation for the first few documents\n","for doc in tfidf_corpus[:5]:\n","    print(doc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kvpn4UkpFlEm"},"outputs":[],"source":["# Looking up the word corresponding to word_id = 0\n","word_id = 5\n","word = dictionary[word_id]\n","print(\"Word corresponding to word_id =\", word_id, \":\", word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUBRKWF-FlEm"},"outputs":[],"source":["# Defining the TF-IDF threshold\n","threshold = 0.05\n","\n","# Iterating through the TF-IDF representation of the documents\n","for doc in tfidf_corpus[:5]:  # Iterating over the first few documents\n","\n","    # Filtering words with TF-IDF scores above the threshold\n","    words_above_threshold = [(dictionary[word_id], tfidf_score) for word_id, tfidf_score in doc if tfidf_score > threshold]\n","\n","    # Printing the words with TF-IDF scores above the threshold\n","    print(\"Words with TF-IDF score > 0.05:\", words_above_threshold)"]},{"cell_type":"markdown","metadata":{"id":"nQRY5j-SFlEn"},"source":["### N-grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSYIvyvDFlEn"},"outputs":[],"source":["# Taking a list of tokens as input and generating n-grams\n","def generate_ngrams(tokens, n=2):\n","    return list(nltk_ngrams(tokens, n))\n","\n","# Creating n-gram model from a corpus\n","def create_ngram_model(corpus, n=2):\n","    ngram_model = defaultdict(int)\n","    for text in corpus:\n","        tokens = word_tokenize(text)\n","        ngrams = generate_ngrams(tokens, n)\n","        for ngram in ngrams:\n","            ngram_model[ngram] += 1\n","    return ngram_model\n","\n","max_texts_to_process = None\n","docs = iterate_pdf_files(pdf_folder)\n","\n","# Concatenating preprocessed tokens for each document into a single list of tokens\n","corpus_tokens = [tokens for sublist in docs.values() for tokens in sublist]\n","\n","# Concatenating tokens into a single string for each document\n","corpus_strings = [' '.join(tokens) for tokens in corpus_tokens]\n","\n","# Creating an n-gram model\n","ngram_model = create_ngram_model(corpus_strings, n=2)\n","\n","# Printing all n-grams\n","print(\"All N-grams:\")\n","for ngram, frequency in ngram_model.items():\n","    print(ngram, \":\", frequency)"]},{"cell_type":"markdown","metadata":{"id":"FpX1MSrzFlEn"},"source":["### BERT topic"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":585,"status":"ok","timestamp":1716073980687,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"YPFqP-sAPpxz"},"outputs":[],"source":["def preprocess_texts(docs):\n","    preprocessed_docs = [' '.join(tokens) for tokens in (doc[0] for doc in docs.values())]\n","    return preprocessed_docs\n","preprocessed_texts = preprocess_texts(docs)"]},{"cell_type":"code","source":["# Initializing the SentenceTransformer model\n","model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# Encoding the preprocessed texts into embeddings\n","embeddings = model.encode(preprocessed_texts, show_progress_bar=True)"],"metadata":{"id":"sKx0kUi1PSHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":44,"metadata":{"id":"bQOtlnsGpIDl","executionInfo":{"status":"ok","timestamp":1716074316150,"user_tz":-180,"elapsed":12282,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"}}},"outputs":[],"source":["# Initializing BERTopic with custom parameters\n","topic_model = BERTopic(min_topic_size=2,\n","                       top_n_words=5,\n","                       n_gram_range=(1, 2),\n","                       calculate_probabilities=True)\n","\n","# Fitting BERTopic on the preprocessed texts and embeddings\n","topics, probs = topic_model.fit_transform(preprocessed_texts,embeddings)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":409,"status":"ok","timestamp":1716074336980,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"47F5yc7N1GvG","outputId":"23e1cde3-3190-4617-e817-19c60363d1bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["    Topic  Count                                               Name  \\\n","0      -1     11                 -1_australia_water_australian_warm   \n","1       0     31                       0_flood_disaster_impact_area   \n","2       1     11                             1_ehe_green_water_cool   \n","3       2     11               2_change_project_water_precipitation   \n","4       3     10                         3_drought_water_event_mean   \n","5       4      9               4_queensland_fire_heatwave_emergency   \n","6       5      7                  5_forecast_mortality_cool_weather   \n","7       6      6  6_anomaly_precipitation_attribution_anthropogenic   \n","8       7      6                     7_heatwave_heatwaves_trend_day   \n","9       8      5                              8_utci_ta_tibet_trend   \n","10      9      5                  9_fishery_chwesl_chwesl event_sea   \n","11     10      3              10_mortality_mortality rate_rate_ewes   \n","\n","                                       Representation  \\\n","0        [australia, water, australian, warm, impact]   \n","1              [flood, disaster, impact, area, water]   \n","2                  [ehe, green, water, cool, element]   \n","3       [change, project, water, precipitation, rise]   \n","4        [drought, water, event, mean, precipitation]   \n","5   [queensland, fire, heatwave, emergency, bushfire]   \n","6           [forecast, mortality, cool, weather, day]   \n","7   [anomaly, precipitation, attribution, anthropo...   \n","8       [heatwave, heatwaves, trend, day, concurrent]   \n","9                        [utci, ta, tibet, trend, hw]   \n","10       [fishery, chwesl, chwesl event, sea, marine]   \n","11      [mortality, mortality rate, rate, ewes, year]   \n","\n","                                  Representative_Docs  \n","0   [april ttrp intergenerational treasury technic...  \n","1   [research wwf equilibrium argument protection ...  \n","2   [substantial occurrence human exposure heatwav...  \n","3   [samoa profile ii profile samoa bank group h s...  \n","4   [article drought typical century could occur e...  \n","5   [weather go wild fuel weather council independ...  \n","6   [article energy demand cool increase past two ...  \n","7   [weather clim author work distribute common at...  \n","8   [article increase trend regional heatwaves hea...  \n","9   [scientific report real time extend range pred...  \n","10  [fish fishery receive november revise february...  \n","11  [future stress muslim pilgrimage hajj project ...  \n"]}],"source":["# Checking the topics identified\n","topic_info = topic_model.get_topic_info()\n","print(topic_info)"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716075072887,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"5ugqoKWi1rXd","outputId":"bd40cc3c-489e-40e4-8d59-3a0bd7fd154a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('queensland', 0.033258904829994565),\n"," ('fire', 0.028118352994422172),\n"," ('heatwave', 0.02370616205229062),\n"," ('emergency', 0.01919691771492977),\n"," ('bushfire', 0.0190237409236173)]"]},"metadata":{},"execution_count":60}],"source":["topic_model.get_topic(4)"]},{"cell_type":"code","source":["# Getting the topic words\n","topic_words = topic_model.get_topics()\n","\n","# Preprocessing the texts\n","preprocessed_texts = preprocess_texts(docs)\n","\n","# Converting preprocessed texts to list of words for coherence model\n","texts = [doc.split() for doc in preprocessed_texts]\n","\n","# Creating a dictionary and corpus for the coherence model\n","dictionary = corpora.Dictionary(texts)\n","corpus = [dictionary.doc2bow(text) for text in texts]\n","\n","coherence_scores = []\n","for topic_id, topic in topic_words.items():\n","    # Preparing the data for coherence model\n","    topics = [[word for word, _ in topic]]\n","\n","    # Computing Coherence Score using Gensim's CoherenceModel for each topic\n","    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n","    coherence_score = coherence_model.get_coherence()\n","\n","    coherence_scores.append((topic_id, coherence_score))\n","\n","# Printing coherence scores for each topic\n","for topic_id, coherence_score in coherence_scores:\n","    print(f'Topic {topic_id}: Coherence Score: {coherence_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5_tZajvNduN","executionInfo":{"status":"ok","timestamp":1716074447540,"user_tz":-180,"elapsed":103538,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"}},"outputId":"430ba893-d52d-4b9f-9c07-81e679587ff2"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic -1: Coherence Score: 0.5961401002909599\n","Topic 0: Coherence Score: 0.5732218838737833\n","Topic 1: Coherence Score: 0.637400711843347\n","Topic 2: Coherence Score: 0.6176852614555182\n","Topic 3: Coherence Score: 0.5656932913561492\n","Topic 4: Coherence Score: 0.8032704067448737\n","Topic 5: Coherence Score: 0.5528102746404895\n","Topic 6: Coherence Score: 0.6779725467346835\n","Topic 7: Coherence Score: 0.6630834536465461\n","Topic 8: Coherence Score: 0.5155617820027103\n","Topic 9: Coherence Score: 0.4692130955977549\n","Topic 10: Coherence Score: 0.7224043979440935\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1z0wteWD685_Z3lYC7_IWSdBx9JOAWIx_","timestamp":1715888955113}]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}