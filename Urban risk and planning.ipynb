{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Um45nKhD-8Q9"},"outputs":[],"source":["pip install PyMuPDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOfYkfIm_HPa"},"outputs":[],"source":["pip install BERTopic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNYsAC2tFlEe"},"outputs":[],"source":["import os\n","import fitz\n","import nltk\n","import re\n","import string\n","import gensim\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import itertools\n","import pprint\n","\n","\n","# For tokenization\n","from nltk.tokenize import word_tokenize\n","nltk.download(\"punkt\")\n","nltk.download('wordnet')\n","\n","# For removing stopwords\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# For lemmatization\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","nltk.download('averaged_perceptron_tagger')\n","\n","# For TF-IDC\n","from gensim import corpora\n","from gensim.models import TfidfModel\n","\n","# For N-grams\n","from collections import defaultdict\n","from nltk.util import ngrams as nltk_ngrams\n","from nltk.tokenize import word_tokenize\n","\n","# For BERT topic\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","\n","# For coherence score\n","from gensim.models.coherencemodel import CoherenceModel"]},{"cell_type":"markdown","metadata":{"id":"tzm7k6FXFlEg"},"source":["### Get text from PDFs"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"gs7u08foFlEh","executionInfo":{"status":"ok","timestamp":1716072274236,"user_tz":-180,"elapsed":269,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"}}},"outputs":[],"source":["def extract_text_from_pdf(pdf_path):\n","    text = \"\"\n","    with fitz.open(pdf_path) as pdf:\n","        for page_num in range(len(pdf)):\n","            page = pdf.load_page(page_num)\n","            text += page.get_text()\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"tiwzhA8vFlEh"},"source":["### Preprocess and vectorize text"]},{"cell_type":"markdown","metadata":{"id":"50jATjIvFlEh"},"source":["*Preprocess pipeline*"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"sB2HEO01CD9g","executionInfo":{"status":"ok","timestamp":1716072277022,"user_tz":-180,"elapsed":309,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"}}},"outputs":[],"source":["# Loading WordNet POS tags for lemmatization\n","def wordnet_pos_tags(treebank_tag):\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":403,"status":"ok","timestamp":1716072279367,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"uc37TnTxFlEi"},"outputs":[],"source":["def txt_preprocess_pipeline(text):\n","\n","    # Removing multiple white spaces and line breaks\n","    clean_txt = re.sub(r'\\n', ' ', text)\n","    clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n","    clean_txt = clean_txt.strip()\n","\n","    # Tokenizing text\n","    tokens = nltk.word_tokenize(clean_txt)\n","\n","    # Loading NLTK stopword list and adding original stopwords\n","    stop_words = set(stopwords.words('english')+['na','na na','city','oecd','urban','red','paper','forum','figgs','enfield','literature','estimate','model','effect','impact','extreme','study','tantly','treme','tsuji','tural','yt','en','ings','factsheet','edney','budin','apps','app','enbel','alth','aunan','basu','user','search','photo','tures','mistry','letter','cdds','ly','national','plan','hhaps','use','initiate','various','public','health','heat','temperature','change','risk', 'increase','health','india','plan', 'brutally','management','part','highlight','world','united','nation','joint', 'cooperation','brief','issue','professor','tsdps','ons','qlw','lfb','qh','qc','humber','osdma','hse','hm','maha','ucs','lok','international','doya','gsdma','cdri','eswd','dluhc','esd','dfe','niosh','naic','desnz','odisha','defra','easac','bouhi','awhp','ndma','rachel','bureau','scientist','adequate','annual','address','amplify','billion','calculation,','activate','adaptation','additionally','acknowledge','afternoon','accordance','act','accord','data','high','ukhsa','osha','mrt','donthu','ensemble','kristina','webinar','qr','percent','interviewee','licker','eea','ccc','conv','january','iiphb','ferguson','ulbrich','trenberth','schär','rcms','nao','germi','veisz','spell','sres','srex','phenomenon','probability','ρ','qsw','moj','nhs','imd','october','academicians','academic','absorb','aapda','nrdc','report','online','across','action','expand','page','u','https','j','g','figure','thank','you','mbove','print','info','escalate','isbn','volume','dr','vol','phd','upon','al', 'e', 'fig', 'cl', 'hap', 'et', 'http', 'ew', 'climate','jun','simon', 'yang','go','copyright','ltd','license','creative','imply','dot', 'dashed','line', 'may', 'either','bi','de', 'la', 'paix', 'tel', 'box', 'fax','front', 'oli', 'scarff', 'getty', 'image', 'back', 'atlas','ness','net', 'san', 'felipe','also','informa', 'tion', 'per', 'cent','due','distri', 'bution','com','bin','thus','km', 'grid','would','christopher','u','aa','aap','aas','ab','abares','abatzoglou','abbs','abc','aberystwyth','doi'])\n","\n","    # Adding custom stopwords directly to the set\n","    custom_stopwords = set(['na','na na','city','oecd','urban','red','paper','forum','figgs','enfield','literature','estimate','model','effect','impact','extreme','study','tantly','treme','tsuji','tural','yt','en','ings','factsheet','edney','budin','apps','app','enbel','alth','aunan','basu','user','search','photo','tures','mistry','letter','cdds','ly','national','plan','hhaps','use','initiate','various','public','health', 'heat', 'temperature', 'change', 'risk', 'increase','health','india','plan', 'brutally','management','part','highlight','world','united','nation','joint', 'cooperation','brief','issue','professor','tsdps','ons','qlw','lfb','qh','qc','humber','osdma','hse','hm','maha','ucs','lok','international','doya','gsdma','cdri','eswd','dluhc','esd','dfe','niosh','naic','desnz','odisha','defra','easac','bouhi','awhp','ndma','rachel','bureau','scientist','adequate','annual','address','amplify','billion','calculation,','activate','adaptation','additionally','acknowledge','afternoon','accordance','act','accord','data','high','ukhsa','osha','mrt','donthu','ensemble','kristina','webinar','qr','percent','interviewee','licker','eea','ccc','conv','january','iiphb','ferguson','ulbrich','trenberth','schär','rcms','nao','germi','veisz','spell','sres','srex','phenomenon','probability','ρ','qsw','moj','nhs','imd','october','academicians','academic','absorb','aapda','nrdc','report','online','across','action','expand','page','u','https','j','g','figure','thank','you','mbove','print','info','escalate','isbn','volume','dr','vol','phd','upon','al', 'e', 'fig', 'cl', 'hap', 'et', 'http', 'ew', 'climate','jun','simon', 'yang','go','copyright','ltd','license','creative','imply','dot', 'dashed','line', 'may', 'either','bi','de', 'la', 'paix', 'tel', 'box', 'fax','front', 'oli', 'scarff', 'getty', 'image', 'back', 'atlas','ness','net', 'san', 'felipe','also','informa', 'tion', 'per', 'cent','due','distri', 'bution','com','bin','thus','km', 'grid','would','christopher','u','aa','aap','aas','ab','abares','abatzoglou','abbs','abc','aberystwyth','doi'])\n","    stop_words.update(custom_stopwords)\n","\n","    # Standardize the text and remove non-alphabetic or stop words\n","    words = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n","\n","    # Defining lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","\n","    # Conducting POS tagging\n","    pos_tags = nltk.pos_tag(words)\n","\n","    # Lemmatizing word-tokens via assigned POS tags\n","    lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]\n","\n","    return lemma_tokens"]},{"cell_type":"markdown","metadata":{"id":"8UFJhhzuFlEk"},"source":["*Iteration function*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45Seszr4OTO9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTrawQzXFlEk"},"outputs":[],"source":["def iterate_pdf_files(pdf_folder, max_texts=None):\n","    # Creating a dictionary to display all documents and tokens together\n","    docs = {}\n","\n","    # Counting to tracking the number of processed texts\n","    text_count = 0\n","\n","    # Iterating over each file in the folder\n","    for filename in os.listdir(pdf_folder):\n","\n","        # Checking whether the files are PDFs\n","        if filename.endswith('.pdf'):\n","\n","            # Creating a path to the PDF files\n","            pdf_path = os.path.join(pdf_folder, filename)\n","\n","            # Extracting the content of files\n","            text = extract_text_from_pdf(pdf_path)\n","\n","            # Applying the preprocessing pipeline to the extracted tex\n","            preprocessed_text = txt_preprocess_pipeline(text)\n","\n","            # Generating a unique name for the document based on the length of the docs dictionary for ease in readibility\n","            doc_name = f'doc{len(docs) + 1}'\n","\n","            # Assigning the preprocessed text content of the current document in the docs dictionary with the document name as the key\n","            docs[doc_name] = [preprocessed_text]\n","\n","    return docs\n","\n","pdf_folder = '/content/drive/MyDrive/urban'\n","\n","docs = iterate_pdf_files(pdf_folder)\n","\n","# Printing each document's tokens as a list\n","for doc_name, tokens in docs.items():\n","    print(f\"{doc_name}: {tokens}\")"]},{"cell_type":"markdown","metadata":{"id":"3OYTkISMFlEl"},"source":["*Token dictionary*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8giyHjwFlEl"},"outputs":[],"source":["# Iterating over the values in the docs dictionary created in the previous step. Each value in the sublist represents the tokens in a certain document.\n","all_preprocessed_text = [token for sublist in docs.values() for token in sublist]\n","\n","# Creating a gensim dictionary to assign an integer ID to each token\n","dictionary = corpora.Dictionary(all_preprocessed_text)\n","print(dictionary)"]},{"cell_type":"markdown","metadata":{"id":"89RhweuBFlEl"},"source":["*BoW representation*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06P76cTgFlEm"},"outputs":[],"source":["# Creating a Bag-of-Words (BoW) representation of the preprocessed text data using the dictionary created in the previous step\n","bow_corpus = [dictionary.doc2bow(text) for text in all_preprocessed_text]\n","pprint.pprint(bow_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16hEUyYZFlEm"},"outputs":[],"source":["print('Number of unique tokens: %d' % len(dictionary))\n","print('Number of documents: %d' % len(bow_corpus))"]},{"cell_type":"markdown","metadata":{"id":"kBkh_rHcFlEm"},"source":["### TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXRqzOxmFlEm"},"outputs":[],"source":["# Creating a TF-IDF model from the Bag-of-Words corpus\n","tfidf_model = TfidfModel(bow_corpus)\n","\n","# Applying the TF-IDF model to the Bag-of-Words corpus to get the TF-IDF representation\n","tfidf_corpus = tfidf_model[bow_corpus]\n","\n","# Printing the TF-IDF representation for the first few documents\n","for doc in tfidf_corpus[:5]:\n","    print(doc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kvpn4UkpFlEm"},"outputs":[],"source":["# Looking up the word corresponding to word_id = 0\n","word_id = 5\n","word = dictionary[word_id]\n","print(\"Word corresponding to word_id =\", word_id, \":\", word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUBRKWF-FlEm"},"outputs":[],"source":["# Defining the TF-IDF threshold\n","threshold = 0.05\n","\n","# Iterating through the TF-IDF representation of the documents\n","for doc in tfidf_corpus[:5]:  # Iterating over the first few documents\n","\n","    # Filtering words with TF-IDF scores above the threshold\n","    words_above_threshold = [(dictionary[word_id], tfidf_score) for word_id, tfidf_score in doc if tfidf_score > threshold]\n","\n","    # Printing the words with TF-IDF scores above the threshold\n","    print(\"Words with TF-IDF score > 0.05:\", words_above_threshold)"]},{"cell_type":"markdown","metadata":{"id":"nQRY5j-SFlEn"},"source":["### N-grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSYIvyvDFlEn"},"outputs":[],"source":["# Taking a list of tokens as input and generating n-grams\n","def generate_ngrams(tokens, n=2):\n","    return list(nltk_ngrams(tokens, n))\n","\n","# Creating n-gram model from a corpus\n","def create_ngram_model(corpus, n=2):\n","    ngram_model = defaultdict(int)\n","    for text in corpus:\n","        tokens = word_tokenize(text)\n","        ngrams = generate_ngrams(tokens, n)\n","        for ngram in ngrams:\n","            ngram_model[ngram] += 1\n","    return ngram_model\n","\n","max_texts_to_process = None\n","docs = iterate_pdf_files(pdf_folder)\n","\n","# Concatenating preprocessed tokens for each document into a single list of tokens\n","corpus_tokens = [tokens for sublist in docs.values() for tokens in sublist]\n","\n","# Concatenating tokens into a single string for each document\n","corpus_strings = [' '.join(tokens) for tokens in corpus_tokens]\n","\n","# Creating an n-gram model\n","ngram_model = create_ngram_model(corpus_strings, n=2)\n","\n","# Printing all n-grams\n","print(\"All N-grams:\")\n","for ngram, frequency in ngram_model.items():\n","    print(ngram, \":\", frequency)"]},{"cell_type":"markdown","metadata":{"id":"FpX1MSrzFlEn"},"source":["### BERT topic"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":344,"status":"ok","timestamp":1716067829170,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"YPFqP-sAPpxz"},"outputs":[],"source":["def preprocess_texts(docs):\n","    preprocessed_docs = [' '.join(tokens) for tokens in (doc[0] for doc in docs.values())]\n","    return preprocessed_docs\n","preprocessed_texts = preprocess_texts(docs)"]},{"cell_type":"code","source":["# Initializing the SentenceTransformer model\n","model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# Encoding the preprocessed texts into embeddings\n","embeddings = model.encode(preprocessed_texts, show_progress_bar=True)"],"metadata":{"id":"sKx0kUi1PSHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"bQOtlnsGpIDl","executionInfo":{"status":"ok","timestamp":1716067965260,"user_tz":-180,"elapsed":24403,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"}}},"outputs":[],"source":["# Initializing BERTopic with custom parameters\n","topic_model = BERTopic(min_topic_size=5,\n","                       top_n_words=5,\n","                       n_gram_range=(1, 2),\n","                       calculate_probabilities=True)\n","\n","# Fitting BERTopic on the preprocessed texts and embeddings\n","topics, probs = topic_model.fit_transform(preprocessed_texts,embeddings)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":282,"status":"ok","timestamp":1716067967804,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"47F5yc7N1GvG","outputId":"7b97c58e-bfb0-4add-a502-6bd2de4552f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["   Topic  Count                                      Name  \\\n","0     -1     21                  -1_london_area_water_use   \n","1      0     14              0_green_cool_building_energy   \n","2      1     13                 1_city_cool_energy_policy   \n","3      2     13           2_exposure_population_city_area   \n","4      3     13             3_roof_energy_cool_resilience   \n","5      4      5  4_community_response_population_heatwave   \n","\n","                                      Representation  \\\n","0                  [london, area, water, use, flood]   \n","1              [green, cool, building, energy, roof]   \n","2                [city, cool, energy, policy, water]   \n","3         [exposure, population, city, area, future]   \n","4            [roof, energy, cool, resilience, green]   \n","5  [community, response, population, heatwave, pe...   \n","\n","                                 Representative_Docs  \n","0  [cool city strategy technology mitigate discus...  \n","1  [check green australia warm city check green a...  \n","2  [island mean east asia city disclosure authori...  \n","3  [projection human exposure dangerous african c...  \n","4  [cool neighborhood nyc new york mayor bill bla...  \n","5  [sustainable city society available april auth...  \n"]}],"source":["# Printing the topics identified\n","topic_info = topic_model.get_topic_info()\n","print(topic_info)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1716072206635,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"5ugqoKWi1rXd","outputId":"fc42708b-3598-4d9b-aaed-2f28741d8274"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('roof', 0.024588811059978836),\n"," ('energy', 0.01789413364865717),\n"," ('cool', 0.016465337393834312),\n"," ('resilience', 0.01598379126088959),\n"," ('green', 0.015255074441601416)]"]},"metadata":{},"execution_count":32}],"source":["topic_model.get_topic(3)"]},{"cell_type":"code","source":["# Getting the topic words\n","topic_words = topic_model.get_topics()\n","\n","# Preprocessing the texts\n","preprocessed_texts = preprocess_texts(docs)\n","\n","# Converting preprocessed texts to list of words for coherence model\n","texts = [doc.split() for doc in preprocessed_texts]\n","\n","# Creating a dictionary and corpus for the coherence model\n","dictionary = corpora.Dictionary(texts)\n","corpus = [dictionary.doc2bow(text) for text in texts]\n","\n","coherence_scores = []\n","for topic_id, topic in topic_words.items():\n","    # Preparing the data for coherence model\n","    topics = [[word for word, _ in topic]]\n","\n","    # Computing Coherence Score using Gensim's CoherenceModel for each topic\n","    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n","    coherence_score = coherence_model.get_coherence()\n","\n","    coherence_scores.append((topic_id, coherence_score))\n","\n","# Printing coherence scores for each topic\n","for topic_id, coherence_score in coherence_scores:\n","    print(f'Topic {topic_id}: Coherence Score: {coherence_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5_tZajvNduN","executionInfo":{"status":"ok","timestamp":1716073415819,"user_tz":-180,"elapsed":45389,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"}},"outputId":"d5f884aa-ec56-47d9-cf60-43f256c8c5b2"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic -1: Coherence Score: 0.5156133370229249\n","Topic 0: Coherence Score: 0.7305625224062327\n","Topic 1: Coherence Score: 0.5094828587124206\n","Topic 2: Coherence Score: 0.5406070659828366\n","Topic 3: Coherence Score: 0.6558229975005739\n","Topic 4: Coherence Score: 0.578000685886485\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1z0wteWD685_Z3lYC7_IWSdBx9JOAWIx_","timestamp":1715888955113}]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}