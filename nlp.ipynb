{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# For tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# For removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# For lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# For TF-IDC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For N-grams\n",
    "from collections import defaultdict\n",
    "from nltk.util import ngrams as nltk_ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# For building LDA model\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# For visualizing the LDA model\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# For evaluating the LDA model\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvisualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        for page_num in range(len(pdf)):\n",
    "            page = pdf.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and vectorize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Preprocess pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading WordNet POS tags for lemmatization\n",
    "def wordnet_pos_tags(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding custom stopwords directly to the set\n",
    "custom_stopwords = set(['g','figure','thank','you','mbove','print','info','escalate','isbn','volume','dr','vol','phd','upon','al', 'e', 'fig', 'cl', 'hap', 'et', 'http', 'ew', 'climate','jun','simon', 'yang','go','copyright','ltd','license','creative','imply','dot', 'dashed','line', 'may', 'either','bi','de', 'la', 'paix', 'tel', 'box', 'fax','front', 'oli', 'scarff', 'getty', 'image', 'back', 'atlas','ness','net', 'san', 'felipe','also','informa', 'tion', 'per', 'cent','due','distri', 'bution','com','bin','thus','km', 'grid','would','christopher','u','aa','aap','aas','ab','abares','abatzoglou','abbs','abc','aberystwyth','doi'])\n",
    "\n",
    "def txt_preprocess_pipeline(text):\n",
    "\n",
    "    # Removing multiple white spaces and line breaks\n",
    "    clean_txt = re.sub(r'\\n', ' ', text)\n",
    "    clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n",
    "    clean_txt = clean_txt.strip()\n",
    "\n",
    "    # Tokenizing text\n",
    "    tokens = nltk.word_tokenize(clean_txt)\n",
    "\n",
    "    # Loading NLTK stopword list and adding original stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(custom_stopwords)\n",
    "\n",
    "    # Standardize the text and remove non-alphabetic or stop words\n",
    "    words = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "    # Defining lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Conducting POS tagging\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # Lemmatizing word-tokens via assigned POS tags\n",
    "    lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]\n",
    "    \n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Iteration function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_pdf_files(pdf_folder, max_texts=None, n=2):\n",
    "    # Creating a dictionary to display all documents and tokens together\n",
    "    docs = {}\n",
    "\n",
    "    # Counting to tracking the number of processed texts\n",
    "    text_count = 0  \n",
    "    \n",
    "    # Iterating over each file in the folder\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "\n",
    "        # Checking wheter the files are PDFs \n",
    "        if filename.endswith('.pdf'):\n",
    "\n",
    "            # Creating a path to the PDF files\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "\n",
    "            # Extracting the content of files\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            # Applying the preprocessing pipeline to the extracted tex\n",
    "            preprocessed_text = txt_preprocess_pipeline(text)\n",
    "\n",
    "            # Generating a unique name for the document based on the length of the docs dictionary for ease in readibility\n",
    "            doc_name = f'doc{len(docs) + 1}'\n",
    "\n",
    "            # Assigning the preprocessed text content of the current document in the docs dictionary with the document name as the key\n",
    "            docs[doc_name] = [preprocessed_text]\n",
    "\n",
    "            # Increasing the text count when finished assigning \n",
    "            text_count += 1\n",
    "            \n",
    "            # Checking if a maximum number of texts has been specified and if the count of processed texts has reached or exceeded this maximum\n",
    "            if max_texts is not None and text_count >= max_texts:  # Check if maximum number of texts reached\n",
    "                break\n",
    "    return docs\n",
    "\n",
    "pdf_folder = \"/Users/aybikesahinoglu/Desktop/NLP-Docs\"\n",
    "\n",
    "# Setting the maximum number of texts to iterate over\n",
    "max_texts_to_process = None\n",
    "docs = iterate_pdf_files(pdf_folder, max_texts=max_texts_to_process)\n",
    "\n",
    "# Printing each document's tokens as a list\n",
    "for doc_name, tokens in docs.items():\n",
    "    print(f\"{doc_name}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Token dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over the values in the docs dictionary created in the previous step. Each value in the sublist represents the tokens in a certain document. \n",
    "all_preprocessed_text = [token for sublist in docs.values() for token in sublist]\n",
    "\n",
    "# Creating a gensim dictionary to assign an integer ID to each token\n",
    "dictionary = corpora.Dictionary(all_preprocessed_text)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*BoW representation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Bag-of-Words (BoW) representation of the preprocessed text data using the dictionary created in the previous step\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in all_preprocessed_text]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BoW corpus to matrix\n",
    "def bow_to_matrix(bow_corpus):\n",
    "    num_docs = len(bow_corpus)\n",
    "    num_terms = len(dictionary)\n",
    "    dtm_matrix = np.zeros((num_docs, num_terms), dtype=np.int32)\n",
    "    \n",
    "    for i, doc in enumerate(bow_corpus):\n",
    "        for id_, count in doc:\n",
    "            dtm_matrix[i, id_] = count\n",
    "    \n",
    "    return dtm_matrix\n",
    "\n",
    "# Convert BoW corpus to matrix\n",
    "dtm_matrix = bow_to_matrix(bow_corpus)\n",
    "\n",
    "# Display the document-term matrix\n",
    "print(dtm_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a list of tokens as input and generating n-grams\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    return list(nltk_ngrams(tokens, n))\n",
    "\n",
    "# Creating n-gram model from a corpus\n",
    "def create_ngram_model(corpus, n=2):\n",
    "    ngram_model = defaultdict(int)\n",
    "    for text in corpus:\n",
    "        tokens = word_tokenize(text)  \n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "        for ngram in ngrams:\n",
    "            ngram_model[ngram] += 1\n",
    "    return ngram_model\n",
    "\n",
    "max_texts_to_process = None\n",
    "docs = iterate_pdf_files(pdf_folder, max_texts=max_texts_to_process)\n",
    "\n",
    "# Concatenating preprocessed tokens for each document into a single list of tokens\n",
    "corpus_tokens = [tokens for sublist in docs.values() for tokens in sublist]\n",
    "\n",
    "# Concatenating tokens into a single string for each document\n",
    "corpus_strings = [' '.join(tokens) for tokens in corpus_tokens]\n",
    "\n",
    "# Creating an n-gram model\n",
    "ngram_model = create_ngram_model(corpus_strings, n=3)\n",
    "\n",
    "# Printing all n-grams\n",
    "print(\"All N-grams:\")\n",
    "for ngram, frequency in ngram_model.items():\n",
    "    print(ngram, \":\", frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting training parameters\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "\n",
    "# Making an index to word dictionary\n",
    "temp = dictionary[0] \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto', # With alpha and eta set to auto, the model learns the sparsity of the document-topic (alpha) and topic-word (eta) distributions\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "top_topics = model.top_topics(bow_corpus)\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = gensimvis.prepare(model, bow_corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
