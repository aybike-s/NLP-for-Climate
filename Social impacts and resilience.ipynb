{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Um45nKhD-8Q9"},"outputs":[],"source":["pip install PyMuPDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOfYkfIm_HPa"},"outputs":[],"source":["pip install BERTopic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNYsAC2tFlEe"},"outputs":[],"source":["import os\n","import fitz\n","import nltk\n","import re\n","import string\n","import gensim\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import itertools\n","import pprint\n","\n","\n","# For tokenization\n","from nltk.tokenize import word_tokenize\n","nltk.download(\"punkt\")\n","nltk.download('wordnet')\n","\n","# For removing stopwords\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# For lemmatization\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","nltk.download('averaged_perceptron_tagger')\n","\n","# For TF-IDC\n","from gensim import corpora\n","from gensim.models import TfidfModel\n","\n","# For N-grams\n","from collections import defaultdict\n","from nltk.util import ngrams as nltk_ngrams\n","from nltk.tokenize import word_tokenize\n","\n","# For BERT topic\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","\n","# For coherence score\n","from gensim.models.coherencemodel import CoherenceModel"]},{"cell_type":"markdown","metadata":{"id":"tzm7k6FXFlEg"},"source":["### Get text from PDFs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gs7u08foFlEh"},"outputs":[],"source":["def extract_text_from_pdf(pdf_path):\n","    text = \"\"\n","    with fitz.open(pdf_path) as pdf:\n","        for page_num in range(len(pdf)):\n","            page = pdf.load_page(page_num)\n","            text += page.get_text()\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"tiwzhA8vFlEh"},"source":["### Preprocess and vectorize text"]},{"cell_type":"markdown","metadata":{"id":"50jATjIvFlEh"},"source":["*Preprocess pipeline*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sB2HEO01CD9g"},"outputs":[],"source":["# Loading WordNet POS tags for lemmatization\n","def wordnet_pos_tags(treebank_tag):\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uc37TnTxFlEi"},"outputs":[],"source":["def txt_preprocess_pipeline(text):\n","\n","    # Removing multiple white spaces and line breaks\n","    clean_txt = re.sub(r'\\n', ' ', text)\n","    clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n","    clean_txt = clean_txt.strip()\n","\n","    # Tokenizing text\n","    tokens = nltk.word_tokenize(clean_txt)\n","\n","    # Loading NLTK stopword list and adding original stopwords\n","    stop_words = set(stopwords.words('english')+['ehst event','ehst','th','lll','reasonable','good','good good','na','na na','city','oecd','urban','red','paper','forum','figgs','enfield','literature','estimate','model','effect','impact','extreme','study','tantly','treme','tsuji','tural','yt','en','ings','factsheet','edney','budin','apps','app','enbel','alth','aunan','basu','user','search','photo','tures','mistry','letter','cdds','ly','national','plan','hhaps','use','initiate','various','public','health','heat','temperature','change','risk', 'increase','health','india','plan', 'brutally','management','part','highlight','world','united','nation','joint', 'cooperation','brief','issue','professor','tsdps','ons','qlw','lfb','qh','qc','humber','osdma','hse','hm','maha','ucs','lok','international','doya','gsdma','cdri','eswd','dluhc','esd','dfe','niosh','naic','desnz','odisha','defra','easac','bouhi','awhp','ndma','rachel','bureau','scientist','adequate','annual','address','amplify','billion','calculation,','activate','adaptation','additionally','acknowledge','afternoon','accordance','act','accord','data','high','ukhsa','osha','mrt','donthu','ensemble','kristina','webinar','qr','percent','interviewee','licker','eea','ccc','conv','january','iiphb','ferguson','ulbrich','trenberth','schär','rcms','nao','germi','veisz','spell','sres','srex','phenomenon','probability','ρ','qsw','moj','nhs','imd','october','academicians','academic','absorb','aapda','nrdc','report','online','across','action','expand','page','u','https','j','g','figure','thank','you','mbove','print','info','escalate','isbn','volume','dr','vol','phd','upon','al', 'e', 'fig', 'cl', 'hap', 'et', 'http', 'ew', 'climate','jun','simon', 'yang','go','copyright','ltd','license','creative','imply','dot', 'dashed','line', 'may', 'either','bi','de', 'la', 'paix', 'tel', 'box', 'fax','front', 'oli', 'scarff', 'getty', 'image', 'back', 'atlas','ness','net', 'san', 'felipe','also','informa', 'tion', 'per', 'cent','due','distri', 'bution','com','bin','thus','km', 'grid','would','christopher','u','aa','aap','aas','ab','abares','abatzoglou','abbs','abc','aberystwyth','doi'])\n","\n","    # Adding custom stopwords directly to the set\n","    custom_stopwords = set(['ehst event','ehst','th','lll','reasonable','good','good good','na','na na','city','oecd','urban','red','paper','forum','figgs','enfield','literature','estimate','model','effect','impact','extreme','study','tantly','treme','tsuji','tural','yt','en','ings','factsheet','edney','budin','apps','app','enbel','alth','aunan','basu','user','search','photo','tures','mistry','letter','cdds','ly','national','plan','hhaps','use','initiate','various','public','health', 'heat', 'temperature', 'change', 'risk', 'increase','health','india','plan', 'brutally','management','part','highlight','world','united','nation','joint', 'cooperation','brief','issue','professor','tsdps','ons','qlw','lfb','qh','qc','humber','osdma','hse','hm','maha','ucs','lok','international','doya','gsdma','cdri','eswd','dluhc','esd','dfe','niosh','naic','desnz','odisha','defra','easac','bouhi','awhp','ndma','rachel','bureau','scientist','adequate','annual','address','amplify','billion','calculation,','activate','adaptation','additionally','acknowledge','afternoon','accordance','act','accord','data','high','ukhsa','osha','mrt','donthu','ensemble','kristina','webinar','qr','percent','interviewee','licker','eea','ccc','conv','january','iiphb','ferguson','ulbrich','trenberth','schär','rcms','nao','germi','veisz','spell','sres','srex','phenomenon','probability','ρ','qsw','moj','nhs','imd','october','academicians','academic','absorb','aapda','nrdc','report','online','across','action','expand','page','u','https','j','g','figure','thank','you','mbove','print','info','escalate','isbn','volume','dr','vol','phd','upon','al', 'e', 'fig', 'cl', 'hap', 'et', 'http', 'ew', 'climate','jun','simon', 'yang','go','copyright','ltd','license','creative','imply','dot', 'dashed','line', 'may', 'either','bi','de', 'la', 'paix', 'tel', 'box', 'fax','front', 'oli', 'scarff', 'getty', 'image', 'back', 'atlas','ness','net', 'san', 'felipe','also','informa', 'tion', 'per', 'cent','due','distri', 'bution','com','bin','thus','km', 'grid','would','christopher','u','aa','aap','aas','ab','abares','abatzoglou','abbs','abc','aberystwyth','doi'])\n","    stop_words.update(custom_stopwords)\n","\n","    # Standardize the text and remove non-alphabetic or stop words\n","    words = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n","\n","    # Defining lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","\n","    # Conducting POS tagging\n","    pos_tags = nltk.pos_tag(words)\n","\n","    # Lemmatizing word-tokens via assigned POS tags\n","    lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]\n","\n","    return lemma_tokens"]},{"cell_type":"markdown","metadata":{"id":"8UFJhhzuFlEk"},"source":["*Iteration function*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45Seszr4OTO9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTrawQzXFlEk"},"outputs":[],"source":["def iterate_pdf_files(pdf_folder):\n","    # Creating a dictionary to display all documents and tokens together\n","    docs = {}\n","\n","    # Counting to tracking the number of processed texts\n","    text_count = 0\n","\n","    # Iterating over each file in the folder\n","    for filename in os.listdir(pdf_folder):\n","\n","        # Checking whether the files are PDFs\n","        if filename.endswith('.pdf'):\n","\n","            # Creating a path to the PDF files\n","            pdf_path = os.path.join(pdf_folder, filename)\n","\n","            # Extracting the content of files\n","            text = extract_text_from_pdf(pdf_path)\n","\n","            # Applying the preprocessing pipeline to the extracted tex\n","            preprocessed_text = txt_preprocess_pipeline(text)\n","\n","            # Generating a unique name for the document based on the length of the docs dictionary for ease in readibility\n","            doc_name = f'doc{len(docs) + 1}'\n","\n","            # Assigning the preprocessed text content of the current document in the docs dictionary with the document name as the key\n","            docs[doc_name] = [preprocessed_text]\n","\n","            # Increasing the text count when finished assigning\n","            text_count += 1\n","\n","    return docs\n","\n","pdf_folder = '/content/drive/MyDrive/social'\n","\n","# Calling the iterate_pdf_files function to get the dictionary of documents\n","docs = iterate_pdf_files(pdf_folder)\n","\n","# Printing each document's tokens as a list\n","for doc_name, tokens in docs.items():\n","    print(f\"{doc_name}: {tokens}\")"]},{"cell_type":"markdown","metadata":{"id":"3OYTkISMFlEl"},"source":["*Token dictionary*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8giyHjwFlEl"},"outputs":[],"source":["# Iterating over the values in the docs dictionary created in the previous step. Each value in the sublist represents the tokens in a certain document.\n","all_preprocessed_text = [token for sublist in docs.values() for token in sublist]\n","\n","# Creating a gensim dictionary to assign an integer ID to each token\n","dictionary = corpora.Dictionary(all_preprocessed_text)\n","print(dictionary)"]},{"cell_type":"markdown","metadata":{"id":"89RhweuBFlEl"},"source":["*BoW representation*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06P76cTgFlEm"},"outputs":[],"source":["# Creating a Bag-of-Words (BoW) representation of the preprocessed text data using the dictionary created in the previous step\n","bow_corpus = [dictionary.doc2bow(text) for text in all_preprocessed_text]\n","pprint.pprint(bow_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16hEUyYZFlEm"},"outputs":[],"source":["print('Number of unique tokens: %d' % len(dictionary))\n","print('Number of documents: %d' % len(bow_corpus))"]},{"cell_type":"markdown","metadata":{"id":"kBkh_rHcFlEm"},"source":["### TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXRqzOxmFlEm"},"outputs":[],"source":["# Creating a TF-IDF model from the Bag-of-Words corpus\n","tfidf_model = TfidfModel(bow_corpus)\n","\n","# Applying the TF-IDF model to the Bag-of-Words corpus to get the TF-IDF representation\n","tfidf_corpus = tfidf_model[bow_corpus]\n","\n","# Printing the TF-IDF representation for the first few documents\n","for doc in tfidf_corpus[:5]:\n","    print(doc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kvpn4UkpFlEm"},"outputs":[],"source":["# Looking up the word corresponding to word_id = 0\n","word_id = 5\n","word = dictionary[word_id]\n","print(\"Word corresponding to word_id =\", word_id, \":\", word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUBRKWF-FlEm"},"outputs":[],"source":["# Defining the TF-IDF threshold\n","threshold = 0.05\n","\n","# Iterating through the TF-IDF representation of the documents\n","for doc in tfidf_corpus[:5]:  # Iterating over the first few documents\n","\n","    # Filtering words with TF-IDF scores above the threshold\n","    words_above_threshold = [(dictionary[word_id], tfidf_score) for word_id, tfidf_score in doc if tfidf_score > threshold]\n","\n","    # Printing the words with TF-IDF scores above the threshold\n","    print(\"Words with TF-IDF score > 0.05:\", words_above_threshold)"]},{"cell_type":"markdown","metadata":{"id":"nQRY5j-SFlEn"},"source":["### N-grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSYIvyvDFlEn"},"outputs":[],"source":["# Taking a list of tokens as input and generating n-grams\n","def generate_ngrams(tokens, n=2):\n","    return list(nltk_ngrams(tokens, n))\n","\n","# Creating n-gram model from a corpus\n","def create_ngram_model(corpus, n=2):\n","    ngram_model = defaultdict(int)\n","    for text in corpus:\n","        tokens = word_tokenize(text)\n","        ngrams = generate_ngrams(tokens, n)\n","        for ngram in ngrams:\n","            ngram_model[ngram] += 1\n","    return ngram_model\n","\n","max_texts_to_process = None\n","docs = iterate_pdf_files(pdf_folder)\n","\n","# Concatenating preprocessed tokens for each document into a single list of tokens\n","corpus_tokens = [tokens for sublist in docs.values() for tokens in sublist]\n","\n","# Concatenating tokens into a single string for each document\n","corpus_strings = [' '.join(tokens) for tokens in corpus_tokens]\n","\n","# Creating an n-gram model\n","ngram_model = create_ngram_model(corpus_strings, n=2)\n","\n","# Printing all n-grams\n","print(\"All N-grams:\")\n","for ngram, frequency in ngram_model.items():\n","    print(ngram, \":\", frequency)"]},{"cell_type":"markdown","metadata":{"id":"FpX1MSrzFlEn"},"source":["### BERT topic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YPFqP-sAPpxz"},"outputs":[],"source":["def preprocess_texts(docs):\n","    preprocessed_docs = [' '.join(tokens) for tokens in (doc[0] for doc in docs.values())]\n","    return preprocessed_docs\n","preprocessed_texts = preprocess_texts(docs)"]},{"cell_type":"code","source":["# Initializing the SentenceTransformer model\n","model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# Encoding the preprocessed texts into embeddings\n","embeddings = model.encode(preprocessed_texts, show_progress_bar=True)"],"metadata":{"id":"sKx0kUi1PSHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQOtlnsGpIDl"},"outputs":[],"source":["# Initializing BERTopic with custom parameters\n","topic_model = BERTopic(min_topic_size=2,\n","                       top_n_words=6,\n","                       n_gram_range=(1, 2),\n","                       calculate_probabilities=True)\n","\n","# Fitting BERTopic on the preprocessed texts and embeddings\n","topics, probs = topic_model.fit_transform(preprocessed_texts,embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1716066123954,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"47F5yc7N1GvG","outputId":"4cd818ff-e85f-4725-e6cb-c268e86cbab4"},"outputs":[{"output_type":"stream","name":"stdout","text":["    Topic  Count                                         Name  \\\n","0      -1      9                   -1_exposure_unit_air_noise   \n","1       0     31        0_disaster_country_development_people   \n","2       1     15            1_worker_work_stress_occupational   \n","3       2     10           2_emission_energy_trade_technology   \n","4       3      8             3_population_exposure_hws_region   \n","5       4      8  4_migration_australian_population_australia   \n","6       5      5            5_drought_vegetation_ozone_global   \n","7       6      4              6_ination_shock_economic_impact   \n","8       7      4                 7_stratum_day_mean_household   \n","9       8      4                   8_event_return_firm_excess   \n","10      9      3                 9_cool_roof_cool roof_access   \n","11     10      3               10_insurance_hate_tweet_farmer   \n","\n","                                       Representation  \\\n","0   [exposure, unit, air, noise, environmental, im...   \n","1   [disaster, country, development, people, loss,...   \n","2   [worker, work, stress, occupational, labour, p...   \n","3   [emission, energy, trade, technology, cool, se...   \n","4   [population, exposure, hws, region, age, morta...   \n","5   [migration, australian, population, australia,...   \n","6      [drought, vegetation, ozone, global, use, lst]   \n","7     [ination, shock, economic, impact, effect, use]   \n","8   [stratum, day, mean, household, electricity, s...   \n","9   [event, return, firm, excess, excess return, day]   \n","10      [cool, roof, cool roof, access, energy, wave]   \n","11  [insurance, hate, tweet, farmer, hate tweet, a...   \n","\n","                                  Representative_Docs  \n","0   [see discussion stats author profile publicati...  \n","1   [reduce poverty protect livelihood build asset...  \n","2   [labour impact workplace workplace environment...  \n","3   [measure impact rural poor woman youth food ag...  \n","4   [gerontologist advance access publication apri...  \n","5   [icon threaten australian tourism council inde...  \n","6   [open access global regional impact representa...  \n","7   [macroeconomic effect shock philippine evidenc...  \n","8   [wave economic activity evidence latin america...  \n","9   [poverty render madagascar highly vulnerable u...  \n","10  [introduction cooler mihir bhatt disaster miti...  \n","11  [social protection temperature experience uk u...  \n"]}],"source":["# Checking the topics identified\n","topic_info = topic_model.get_topic_info()\n","print(topic_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1716066977022,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"},"user_tz":-180},"id":"5ugqoKWi1rXd","outputId":"ed3ffd04-c225-4747-ebdf-b5fdfa19258a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('worker', 0.0407689096562),\n"," ('work', 0.0359700334860189),\n"," ('stress', 0.028769548533978662),\n"," ('occupational', 0.015786650454256947),\n"," ('labour', 0.014128357263912606),\n"," ('productivity', 0.013997133581472936)]"]},"metadata":{},"execution_count":153}],"source":["topic_model.get_topic(1)"]},{"cell_type":"code","source":["# Getting the topic words\n","topic_words = topic_model.get_topics()\n","\n","# Preprocessing the texts\n","preprocessed_texts = preprocess_texts(docs)\n","\n","# Converting preprocessed texts to list of words for coherence model\n","texts = [doc.split() for doc in preprocessed_texts]\n","\n","# Creating a dictionary and corpus for the coherence model\n","dictionary = corpora.Dictionary(texts)\n","corpus = [dictionary.doc2bow(text) for text in texts]\n","\n","coherence_scores = []\n","for topic_id, topic in topic_words.items():\n","    # Preparing the data for coherence model\n","    topics = [[word for word, _ in topic]]\n","\n","    # Computing Coherence Score using Gensim's CoherenceModel for each topic\n","    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n","    coherence_score = coherence_model.get_coherence()\n","\n","    coherence_scores.append((topic_id, coherence_score))\n","\n","# Printing coherence scores for each topic\n","for topic_id, coherence_score in coherence_scores:\n","    print(f'Topic {topic_id}: Coherence Score: {coherence_score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5_tZajvNduN","executionInfo":{"status":"ok","timestamp":1716066229566,"user_tz":-180,"elapsed":101240,"user":{"displayName":"Aybike Sahinoglu","userId":"01606546040679719892"}},"outputId":"a2597f37-b9d9-469b-df42-2a1d6c9f948e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic -1: Coherence Score: 0.5838164243312369\n","Topic 0: Coherence Score: 0.49169192606711826\n","Topic 1: Coherence Score: 0.8309315789563628\n","Topic 2: Coherence Score: 0.6224394017065169\n","Topic 3: Coherence Score: 0.5646214306496297\n","Topic 4: Coherence Score: 0.5743408169877359\n","Topic 5: Coherence Score: 0.40617713979120246\n","Topic 6: Coherence Score: 0.37400189130846867\n","Topic 7: Coherence Score: 0.6061261761116336\n","Topic 8: Coherence Score: 0.6528843252746699\n","Topic 9: Coherence Score: 0.6687805530825827\n","Topic 10: Coherence Score: 0.11205792335028408\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1z0wteWD685_Z3lYC7_IWSdBx9JOAWIx_","timestamp":1715888955113}]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}